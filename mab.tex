%% ----------------------------------------------------------------
%% cyk.tex -- main
%% ----------------------------------------------------------------

\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\floatname{algorithm}{Algorithm}
\newcommand{\todo}[1]{{\color{red}#1}}

\title{
    Assignment 2 -- Reinforcement Learning\\
    \vspace{3em}
    {\large Umeå University \\
    Artificial Intelligence -- Methods and Applications (5DV181)}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\textwidth]{pong}
    \end{figure}
    \vspace{2em}
}
\author{Salome Müller, \texttt{mcs21smr}\\ Lucas Galery Käser, \texttt{mcs21lgr}}
\date{\today}

\begin{document}

%% Make the title
\maketitle

%\todo{add pong gif/picture to title page}

\pagebreak

%% start the content for multi armed bandit followed by pong
\section{Multi-Armed Bandit}\label{sec:mab}
We start by describing the improvements we did on the multi-armed bandit, the results we achieved and further work that could be done to improve the bandit.

\subsection{Improving the Bandit}\label{subsec:improving-the-bandit}
In order to improve the bandit, we first introduced $\epsilon$-decay.
This way, thirteen out of twenty simulations could be won.
Since our bandit must win at least sixteen of the simulations, we introduced another improvement method, namely the \textit{sliding window}.

\subsubsection{Sliding Window}
We implemented one sliding window over all arms.
The window size is constant and determines the amount of past actions to be considered by the agent.
%We added a constant (\texttt{WINDOW\_SIZE}) defining how big the window is, i.e., the amount of past actions that are to be considered by the agent.
We implemented the sliding window concept using two arrays: one for the frequencies of the arms and one for the sum of rewards.
The expected value of each arm is then computed from the values in those two arrays.

This method ensures that all arms are regularly tested.
If one window per arm or no window at all is used, arms that give a bad reward for the first pull are likely to be never tried again, and possible changes in the environment can not be adapted to.
With the sliding window over all arms this is prevented, and a good balance between exploitation and exploration can be achieved.

\subsubsection{Tuning Parameters}
The siding window turned out to yield the best results with a size of 38.
With that value, all twenty simulations are won.
When only using $\epsilon$-decay, the best parameters are a decay of 0.97 after each iteration, until epsilon reaches a minimal value of 0.01.
All these parameter values were obtained by running the program with different values and choosing the ones that yielded the highest amount of won simulations.


\subsection{Results}\label{subsec:mab-results}
With the sliding window, our bandit wins twenty out of twenty simulations.
This shows that it finds good arms quickly, and also adapts to change in the environment fast.

Using $\epsilon$-decay without the sliding window led to the agent winning thirteen of the simulations, but the bandit focused mostly on one arm.
The bad rewards some arms got in the first simulation(s) led them to hardly be explored further.
The combination of both the sliding window and $\epsilon$-decay led to the bandit winning seventeen out of the twenty simulations.
We assume this is due to the fact that the sliding window alone provides a good exploration-exploitation balance already, and the $\epsilon$-decay interferes with that.


\subsection{Further Optimization}\label{subsec:mab-further-optimization}
We tuned the parameters of our agent by hand, they probably could be improved using a script that tests a wider range of values systematically.
To further optimize, it would be interesting to run our agent against other optimized agents.


\section{Pong}\label{sec:pong}
Next, we describe how we improved our pong-playing agent, what results our agent was able to achieve, and how it could be further optimized.

\subsection{Improving the Agent}\label{subsec:improving-the-agent}
Improving the agent consisted mostly of two parts: implementing a reasonable abstraction for the state space and tuning all the parameters.

\subsubsection{Choosing a State-Space Abstraction}
Our final state-space abstraction was developed iteratively.
We started by deciding that we only want to focus on the positional data for our own agent and therefore disregard the two data points for the opponent's position.
Also, since our agent does not move horizontally, we only used its vertical coordinate.
To simplify things even further, we decided to look only at the relative vertical position of our agent's paddle and the ball:

\[\mathit{rel\_pos} = \begin{cases}
                          1 & \mathit{paddle\_pos} > \mathit{ball\_pos} \\
                          0 & \mathit{paddle\_pos} = \mathit{ball\_pos} \\
                          -1 & \mathit{paddle\_pos} < \mathit{ball\_pos}
\end{cases}
\]

Additionally, we added both coordinates of the ball to the state representation, rounded to one decimal point.
Finally, our state representation contains an integer between 0 and 5, representing one of the six possible directions of the ball.
After all this simplification, a game state had the following form,\[\texttt{\lq[-1, 0.5, 0.5, 2]\rq},\] which in one of our experiments led the Q-table to have 666 entries after 350 episodes.

\subsubsection{Tuning Parameters}
In order to start with the parameter tuning from a reasonable point, we used the $\alpha$, $\gamma$ and $\epsilon$ values from the provided \href{https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/}{tutorial} for our agent.
We discovered that with an $\epsilon$ value of 0.2, our agent performed even better, so we changed it.
For the minimum $\epsilon$ and the $\epsilon$-decay values, we decided to go with the same values as we used in the multi-armed bandit.
As our agent was already performing very good at that stage, achieving a score of around 2000 for \textit{wins - losses} after about 350 episodes, we decided to not change the parameters further.
Finally, our parameter tuning resolved in the following values for our parameters:
\[\alpha = 0.1, \gamma = 0.6, \epsilon = 0.2, \epsilon_\mathit{min} = 0.01, \epsilon_\mathit{decay} = 0.97.\]

\subsection{Results}\label{subsec:pong-results}
With our state space discretization and our tuned parameters, our pong agent was able to achieve a \textit{wins - losses} score of around 2000 after 350 episodes.
Since the goal was a score of at least 1000 at that stage, we were satisfied with our results and did not improve the agent further.

\subsection{Further Optimization}\label{subsec:pong-further-optimization}
Our pong agent possibly could be improved by testing out combinations of a wider range of parameters systematically with a script.
Also, testing alternative state space abstractions could improve the agent.


\end{document}